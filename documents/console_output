(.venv) C:\Users\dugie\PycharmProjects\BrazilPipeline>uvicorn api.main:app --host 0.0.0.0 --port 8000
Added C:\Users\dugie\PycharmProjects\BrazilPipeline to Python path
Added C:\Users\dugie\PycharmProjects\BrazilPipeline\pipeline to Python path
INFO:     Started server process [41072]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:7303 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:7303 - "GET /openapi.json HTTP/1.1" 200 OK
2025-03-17 17:14:48,222 - api.main - INFO - Loading run_pipeline from: C:\Users\dugie\PycharmProjects\BrazilPipeline\pipeline\run_pipeline.py
2025-03-17 17:14:55,844 - api.main - INFO - Starting pipeline run
Warning: MLflow setup error: autolog() got an unexpected keyword argument 'log_feature_importance'

==================================================
STEP 1: DATA INGESTION
==================================================
Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce
Dataset downloaded successfully.
Loaded datasets - orders: 99441, products: 32951
Final dataset shape: (112650, 23)

==================================================
STEP 2: DATA VALIDATION
==================================================
Warning: 8427 price outliers detected
Warning: Missing values in categorical columns: {'product_category_name_english': 1627, 'payment_type': 3}
Warning: 10225 duplicated product-order combinations
Dataset: 112650 rows | Price: $0.85-$6735.00 (median: $74.99) | 71 categories
Data validation passed

==================================================
STEP 3: DATA TRANSFORMATION
==================================================
Starting data transformation...
Handling missing values...
Handling outliers...
Removed 211 price outliers
Applying Label Encoding to categorical variables...
Finalizing features...
Final dataset shape: (112439, 18), 16 features
Split data: 78707 training samples, 33732 test samples

==================================================
STEP 4: MODEL TRAINING
==================================================
Starting model training with GPU acceleration for XGBoost and Neural Network...
Training data: 78707 samples, 16 features

Training Decision Tree model...
ğŸƒ View run Decision Tree at: http://127.0.0.1:8080/#/experiments/9/runs/9976f7c222754ce39fa50379e01612d8
ğŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/9
Decision Tree - Test RÂ²: 0.3804, RMSE: 128.24, MAE: 57.69, MSE: 16446.75, MAPE: 71.82%

Training Random Forest model...
ğŸƒ View run Random Forest at: http://127.0.0.1:8080/#/experiments/9/runs/c314b40e9aca4f3c9f1016828098987c
ğŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/9
Random Forest - Test RÂ²: 0.6408, RMSE: 97.65, MAE: 44.71, MSE: 9536.07, MAPE: 57.99%

Training XGBoost model with enhanced GPU acceleration...
XGBoost CV RMSE: 80.6951 at 500 rounds
XGBoost - Test RÂ²: 0.7685, RMSE: 78.40, MAE: 29.53, MSE: 6146.16, MAPE: 33.55%
XGBoost - Best boosting rounds: 500

Training Neural Network model with GPU optimization...
Epoch 1/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 5ms/step - loss: 39687.4453 - mae: 118.2004 - val_loss: 35877.5781 - val_mae: 119.9653 - learning_rate: 0.0010
Epoch 2/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 35626.0664 - mae: 111.8373 - val_loss: 31372.8691 - val_mae: 112.6740 - learning_rate: 0.0010
Epoch 3/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 30279.7656 - mae: 100.6849 - val_loss: 25811.0039 - val_mae: 99.0352 - learning_rate: 0.0010
Epoch 4/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 25114.3828 - mae: 88.9062 - val_loss: 20829.8223 - val_mae: 81.7110 - learning_rate: 0.0010
Epoch 5/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 21372.0449 - mae: 78.5948 - val_loss: 17501.5059 - val_mae: 68.1724 - learning_rate: 0.0010
Epoch 6/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 19016.0273 - mae: 70.7693 - val_loss: 15435.3896 - val_mae: 59.7744 - learning_rate: 0.0010
Epoch 7/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 17497.8008 - mae: 66.1188 - val_loss: 14450.7021 - val_mae: 57.1156 - learning_rate: 0.0010
Epoch 8/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 16862.2441 - mae: 64.8326 - val_loss: 13898.4414 - val_mae: 56.9787 - learning_rate: 0.0010
Epoch 9/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 16288.6152 - mae: 64.4450 - val_loss: 13671.5547 - val_mae: 57.9880 - learning_rate: 0.0010
Epoch 10/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 16028.5723 - mae: 64.7036 - val_loss: 13337.2646 - val_mae: 57.9275 - learning_rate: 0.0010
Epoch 11/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 15869.9893 - mae: 64.6070 - val_loss: 13614.5342 - val_mae: 58.2866 - learning_rate: 0.0010
Epoch 12/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 15858.5889 - mae: 64.8917 - val_loss: 13500.8486 - val_mae: 58.4891 - learning_rate: 0.0010
Epoch 13/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 15597.9385 - mae: 64.6407 - val_loss: 13179.5000 - val_mae: 57.5764 - learning_rate: 0.0010
Epoch 14/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 15486.1787 - mae: 64.4518 - val_loss: 13302.4932 - val_mae: 58.1648 - learning_rate: 0.0010
Epoch 15/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 15255.1572 - mae: 64.0499 - val_loss: 13010.8457 - val_mae: 57.3737 - learning_rate: 0.0010
Epoch 16/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 15353.0508 - mae: 63.9708 - val_loss: 12955.4932 - val_mae: 58.2711 - learning_rate: 0.0010
Epoch 17/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 15065.1357 - mae: 63.6750 - val_loss: 12796.0332 - val_mae: 57.8540 - learning_rate: 0.0010
Epoch 18/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 15231.9219 - mae: 63.7967 - val_loss: 12744.5410 - val_mae: 56.8399 - learning_rate: 0.0010
Epoch 19/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14949.4570 - mae: 63.2116 - val_loss: 12602.7031 - val_mae: 55.9631 - learning_rate: 0.0010
Epoch 20/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14771.9023 - mae: 62.8784 - val_loss: 12837.0518 - val_mae: 57.2725 - learning_rate: 0.0010
Epoch 21/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14821.7236 - mae: 62.8599 - val_loss: 12455.4805 - val_mae: 55.7807 - learning_rate: 0.0010
Epoch 22/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14616.6533 - mae: 62.7929 - val_loss: 12636.1299 - val_mae: 57.6377 - learning_rate: 0.0010
Epoch 23/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14706.5654 - mae: 62.8445 - val_loss: 12718.7285 - val_mae: 57.9558 - learning_rate: 0.0010
Epoch 24/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14209.2393 - mae: 62.2335 - val_loss: 12487.1299 - val_mae: 55.9134 - learning_rate: 0.0010
Epoch 25/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14243.8994 - mae: 62.0054 - val_loss: 12601.1035 - val_mae: 56.8917 - learning_rate: 0.0010
Epoch 26/100
245/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 14323.0391 - mae: 62.2022 
Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14320.5312 - mae: 62.1996 - val_loss: 12570.9492 - val_mae: 56.2783 - learning_rate: 0.0010
Epoch 27/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 14016.5039 - mae: 61.8289 - val_loss: 12245.6465 - val_mae: 54.9555 - learning_rate: 5.0000e-04
Epoch 28/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13765.1064 - mae: 61.2456 - val_loss: 12023.1104 - val_mae: 55.5792 - learning_rate: 5.0000e-04
Epoch 29/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13784.0186 - mae: 61.5582 - val_loss: 12117.4854 - val_mae: 55.0693 - learning_rate: 5.0000e-04
Epoch 30/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13390.7100 - mae: 60.7581 - val_loss: 11998.3838 - val_mae: 55.6211 - learning_rate: 5.0000e-04
Epoch 31/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13615.8799 - mae: 61.1237 - val_loss: 11960.2041 - val_mae: 55.2765 - learning_rate: 5.0000e-04
Epoch 32/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13768.0898 - mae: 61.1256 - val_loss: 11941.0625 - val_mae: 54.9391 - learning_rate: 5.0000e-04
Epoch 33/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13398.2080 - mae: 60.7175 - val_loss: 12184.9912 - val_mae: 54.7735 - learning_rate: 5.0000e-04
Epoch 34/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13588.8271 - mae: 61.1662 - val_loss: 11931.4658 - val_mae: 54.6851 - learning_rate: 5.0000e-04
Epoch 35/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13488.7617 - mae: 60.8276 - val_loss: 12032.5312 - val_mae: 55.4572 - learning_rate: 5.0000e-04
Epoch 36/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13476.5664 - mae: 60.8193 - val_loss: 11989.0215 - val_mae: 54.6415 - learning_rate: 5.0000e-04
Epoch 37/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13384.9756 - mae: 60.6947 - val_loss: 11811.5439 - val_mae: 54.8439 - learning_rate: 5.0000e-04
Epoch 38/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13425.4395 - mae: 60.8117 - val_loss: 11961.1426 - val_mae: 54.3803 - learning_rate: 5.0000e-04
Epoch 39/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13437.5840 - mae: 60.5644 - val_loss: 11944.4658 - val_mae: 54.8185 - learning_rate: 5.0000e-04
Epoch 40/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13113.3438 - mae: 60.3915 - val_loss: 11794.6260 - val_mae: 54.9465 - learning_rate: 5.0000e-04
Epoch 41/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13288.4033 - mae: 60.6651 - val_loss: 12026.3184 - val_mae: 54.8549 - learning_rate: 5.0000e-04
Epoch 42/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 13081.6143 - mae: 60.5343 - val_loss: 11825.8770 - val_mae: 55.1669 - learning_rate: 5.0000e-04
Epoch 43/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 13061.8447 - mae: 60.3297 - val_loss: 12025.8760 - val_mae: 54.7262 - learning_rate: 5.0000e-04
Epoch 44/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13319.2734 - mae: 60.5479 - val_loss: 11783.8086 - val_mae: 54.7852 - learning_rate: 5.0000e-04
Epoch 45/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13193.3271 - mae: 60.2894 - val_loss: 11697.7363 - val_mae: 54.0560 - learning_rate: 5.0000e-04
Epoch 46/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 13216.6924 - mae: 60.4744 - val_loss: 11792.7227 - val_mae: 54.3652 - learning_rate: 5.0000e-04
Epoch 47/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 13108.2002 - mae: 60.2405 - val_loss: 11770.8281 - val_mae: 53.6865 - learning_rate: 5.0000e-04
Epoch 48/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12814.3223 - mae: 59.8940 - val_loss: 11764.1514 - val_mae: 54.2134 - learning_rate: 5.0000e-04
Epoch 49/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 13023.4531 - mae: 59.9361 - val_loss: 11640.4424 - val_mae: 54.0233 - learning_rate: 5.0000e-04
Epoch 50/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 13051.0713 - mae: 59.9824 - val_loss: 11788.8086 - val_mae: 54.4002 - learning_rate: 5.0000e-04
Epoch 51/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12915.5869 - mae: 59.9536 - val_loss: 11690.8809 - val_mae: 54.4228 - learning_rate: 5.0000e-04
Epoch 52/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12769.1670 - mae: 59.5748 - val_loss: 11661.6152 - val_mae: 54.9096 - learning_rate: 5.0000e-04
Epoch 53/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12957.7334 - mae: 59.8089 - val_loss: 11644.1914 - val_mae: 54.2131 - learning_rate: 5.0000e-04
Epoch 54/100
234/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 12657.1973 - mae: 59.4391 
Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12655.5205 - mae: 59.4446 - val_loss: 11752.2314 - val_mae: 54.3412 - learning_rate: 5.0000e-04
Epoch 55/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12558.3291 - mae: 59.1703 - val_loss: 11603.9238 - val_mae: 53.5534 - learning_rate: 2.5000e-04
Epoch 56/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12855.6992 - mae: 59.7511 - val_loss: 11574.2656 - val_mae: 54.0465 - learning_rate: 2.5000e-04
Epoch 57/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12676.5479 - mae: 59.5431 - val_loss: 11486.3545 - val_mae: 53.5225 - learning_rate: 2.5000e-04
Epoch 58/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12509.8662 - mae: 59.3240 - val_loss: 11413.3682 - val_mae: 53.3316 - learning_rate: 2.5000e-04
Epoch 59/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12628.4541 - mae: 59.1944 - val_loss: 11537.2920 - val_mae: 53.4245 - learning_rate: 2.5000e-04
Epoch 60/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12523.8301 - mae: 59.2473 - val_loss: 11538.9795 - val_mae: 53.7613 - learning_rate: 2.5000e-04
Epoch 61/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12440.8389 - mae: 59.0201 - val_loss: 11589.2412 - val_mae: 53.5289 - learning_rate: 2.5000e-04
Epoch 62/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12660.4814 - mae: 59.4467 - val_loss: 11504.0615 - val_mae: 53.5485 - learning_rate: 2.5000e-04
Epoch 63/100
239/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 4ms/step - loss: 12522.7422 - mae: 59.0778 
Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12512.4541 - mae: 59.0730 - val_loss: 11458.2744 - val_mae: 53.7024 - learning_rate: 2.5000e-04
Epoch 64/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12182.8604 - mae: 58.8704 - val_loss: 11461.2832 - val_mae: 53.5789 - learning_rate: 1.2500e-04
Epoch 65/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12432.5811 - mae: 58.9914 - val_loss: 11459.3213 - val_mae: 53.7802 - learning_rate: 1.2500e-04
Epoch 66/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12278.9746 - mae: 58.8294 - val_loss: 11361.2939 - val_mae: 53.4122 - learning_rate: 1.2500e-04
Epoch 67/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12379.0078 - mae: 59.1107 - val_loss: 11429.4902 - val_mae: 53.3533 - learning_rate: 1.2500e-04
Epoch 68/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12241.3994 - mae: 58.7693 - val_loss: 11385.9150 - val_mae: 53.6755 - learning_rate: 1.2500e-04
Epoch 69/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12146.4902 - mae: 58.7794 - val_loss: 11389.2207 - val_mae: 53.2590 - learning_rate: 1.2500e-04
Epoch 70/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12159.0312 - mae: 58.5946 - val_loss: 11401.5098 - val_mae: 53.2834 - learning_rate: 1.2500e-04
Epoch 71/100
242/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 12225.6533 - mae: 58.6220 
Epoch 71: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12223.3711 - mae: 58.6223 - val_loss: 11434.7930 - val_mae: 53.3328 - learning_rate: 1.2500e-04
Epoch 72/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12377.2236 - mae: 58.8810 - val_loss: 11456.4932 - val_mae: 53.3852 - learning_rate: 6.2500e-05
Epoch 73/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12311.3721 - mae: 58.8057 - val_loss: 11426.7764 - val_mae: 53.3397 - learning_rate: 6.2500e-05
Epoch 74/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12306.3945 - mae: 58.8874 - val_loss: 11435.7012 - val_mae: 53.4721 - learning_rate: 6.2500e-05
Epoch 75/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12215.4639 - mae: 58.6743 - val_loss: 11374.4668 - val_mae: 53.4137 - learning_rate: 6.2500e-05
Epoch 76/100
242/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 12304.6240 - mae: 58.5457 
Epoch 76: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12298.8477 - mae: 58.5456 - val_loss: 11384.4004 - val_mae: 53.3391 - learning_rate: 6.2500e-05
Epoch 77/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12288.3975 - mae: 58.9757 - val_loss: 11381.1816 - val_mae: 53.3481 - learning_rate: 3.1250e-05
Epoch 78/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12150.9668 - mae: 58.6795 - val_loss: 11359.6025 - val_mae: 53.2817 - learning_rate: 3.1250e-05
Epoch 79/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12320.3867 - mae: 58.7689 - val_loss: 11379.6885 - val_mae: 53.2752 - learning_rate: 3.1250e-05
Epoch 80/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12120.5420 - mae: 58.7781 - val_loss: 11374.0312 - val_mae: 53.2045 - learning_rate: 3.1250e-05
Epoch 81/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12326.9619 - mae: 58.8798 - val_loss: 11362.8955 - val_mae: 53.2594 - learning_rate: 3.1250e-05
Epoch 82/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12360.2695 - mae: 58.8841 - val_loss: 11364.8809 - val_mae: 53.2470 - learning_rate: 3.1250e-05
Epoch 83/100
233/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 11934.1299 - mae: 58.7948 
Epoch 83: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 11922.5830 - mae: 58.7730 - val_loss: 11367.9609 - val_mae: 53.2168 - learning_rate: 3.1250e-05
Epoch 84/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 11929.8232 - mae: 58.1579 - val_loss: 11357.9775 - val_mae: 53.2479 - learning_rate: 1.5625e-05
Epoch 85/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12111.6152 - mae: 58.6919 - val_loss: 11322.4775 - val_mae: 53.2490 - learning_rate: 1.5625e-05
Epoch 86/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12311.6201 - mae: 58.7535 - val_loss: 11342.6387 - val_mae: 53.2536 - learning_rate: 1.5625e-05
Epoch 87/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12292.9727 - mae: 58.5885 - val_loss: 11343.0635 - val_mae: 53.3562 - learning_rate: 1.5625e-05
Epoch 88/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12344.3506 - mae: 58.7923 - val_loss: 11320.4082 - val_mae: 53.3284 - learning_rate: 1.5625e-05
Epoch 89/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12207.2041 - mae: 58.7007 - val_loss: 11303.1074 - val_mae: 53.0750 - learning_rate: 1.5625e-05
Epoch 90/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12104.3135 - mae: 58.4592 - val_loss: 11339.7334 - val_mae: 53.0564 - learning_rate: 1.5625e-05
Epoch 91/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12136.1182 - mae: 58.6185 - val_loss: 11312.3174 - val_mae: 53.0480 - learning_rate: 1.5625e-05
Epoch 92/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12233.6631 - mae: 58.7835 - val_loss: 11321.2031 - val_mae: 53.1409 - learning_rate: 1.5625e-05
Epoch 93/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12183.1045 - mae: 58.7840 - val_loss: 11332.7666 - val_mae: 53.1810 - learning_rate: 1.5625e-05
Epoch 94/100
240/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 0s 3ms/step - loss: 12278.1455 - mae: 58.6973 
Epoch 94: ReduceLROnPlateau reducing learning rate to 1e-05.
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 3ms/step - loss: 12266.7041 - mae: 58.6906 - val_loss: 11326.5830 - val_mae: 53.0980 - learning_rate: 1.5625e-05
Epoch 95/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12270.5605 - mae: 58.8127 - val_loss: 11332.6279 - val_mae: 53.1197 - learning_rate: 1.0000e-05
Epoch 96/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12239.1826 - mae: 58.8336 - val_loss: 11300.0420 - val_mae: 53.1078 - learning_rate: 1.0000e-05
Epoch 97/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12205.6455 - mae: 58.7067 - val_loss: 11309.7314 - val_mae: 53.1007 - learning_rate: 1.0000e-05
Epoch 98/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 12495.0977 - mae: 58.8778 - val_loss: 11311.0391 - val_mae: 53.1609 - learning_rate: 1.0000e-05
Epoch 99/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 5ms/step - loss: 12242.5117 - mae: 58.6204 - val_loss: 11317.7295 - val_mae: 53.1609 - learning_rate: 1.0000e-05
Epoch 100/100
246/246 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 1s 4ms/step - loss: 12298.9385 - mae: 58.6480 - val_loss: 11326.5820 - val_mae: 53.1214 - learning_rate: 1.0000e-05
Restoring model weights from the end of the best epoch: 96.
2025/03/17 17:19:51 WARNING mlflow.utils.autologging_utils: MLflow tensorflow autologging is known to be compatible with 2.7.4 <= tensorflow <= 2.18.0, but the installed version is 2.19.0. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a compatible version, or try upgrading MLflow. 
ğŸƒ View run Neural Network at: http://127.0.0.1:8080/#/experiments/9/runs/44f3d6c8a002466e92522b540fb3b99b
ğŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/9
Neural Network - Validation RÂ²: 0.5307, RMSE: 106.30, MAE: 53.11, MSE: 11299.74, MAPE: 76.32%
Neural Network - Test RÂ²: 0.5258, RMSE: 112.19, MAE: 54.22, MSE: 12586.67, MAPE: 75.95%

Best model: xgboost_gpu

Model Performance Comparison:
            Model   Test RÂ²   Test RMSE   Test MAE  Test MAPE (%)
2     xgboost_gpu  0.768458   78.397472  29.530198      33.548570
1   random_forest  0.640751   97.652804  44.707408      57.993010
3  neural_network  0.525827  112.190346  54.224077      75.954695
0   decision_tree  0.380408  128.244873  57.692297      71.819335

Top 10 Important Features (xgboost_gpu):
                          feature    importance
0                   freight_value  73315.023438
13           payment_installments  69806.539062
9                    seller_state  67027.453125
8   product_category_name_english  59623.777344
4                product_weight_g  49658.882812
10                    seller_city  44314.691406
2      product_description_lenght  42869.359375
6               product_height_cm  36914.273438
7                product_width_cm  36713.949219
5               product_length_cm  34735.191406

==================================================
STEP 5: MODEL EVALUATION
==================================================
Starting model evaluation for GPU-accelerated models...
Using custom prediction function for xgboost_gpu
Model: xgboost_gpu
RÂ² Score: 0.7685
RMSE: 78.3975
MAE: 29.5302
MAPE: 33.55%

Error by Price Range:
             count  mean_abs_error  mean_pct_error
price_range
0-50         11867       14.314787       54.927606
50-100        9832       18.001039       24.381167
100-200       8059       26.731442       18.821676
200-500       3054       62.886226       20.593221
500-1000       682      179.538437       25.280972
1000plus       238      501.363014       34.880798

==================================================
PIPELINE COMPLETED SUCCESSFULLY
==================================================
Best model: xgboost_gpu
RÂ²: 0.7685 | RMSE: R$78.40
==================================================
ğŸƒ View run capable-fox-701 at: http://127.0.0.1:8080/#/experiments/9/runs/9109568f42ac4d10a2faf7ec7eeb8f49
ğŸ§ª View experiment at: http://127.0.0.1:8080/#/experiments/9
INFO:     127.0.0.1:7303 - "POST /predict HTTP/1.1" 200 OK
INFO:     127.0.0.1:8692 - "GET / HTTP/1.1" 200 OK
